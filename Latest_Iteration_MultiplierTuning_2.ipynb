{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This version adjusts results using average predicted prices from a macro model.\n",
    "\n",
    "I'm not sure I've succeeded in doing better than magic numbers, because I have a new trick now:\n",
    "\n",
    "    y_rescaled = m  +  (y-ybar) * (1+ybar-m)\n",
    "\n",
    "(everything in logs).  `m` is the new mean, which comes from the macro model, and the `1+ybar-m` is meant to offset the scale effect of the mean shift.  (The mean shift has a scale effect because we're working in logs, and shifting the mean of the log shifts both the mean and the scale of the raw numbers.  We wanted to shift the mean but not necessarily the scale.)  As far as calculation goes, it was the simplest way I could think to offset the scale effect, but it doesn't have a clear analytical meaning, and I wouldn't be doing it if it didn't improve the results.  But at least it doesn't involve any magic numbers.\n",
    "\n",
    "There is also a provision for applying micro and macro humility factors . The micro humility factor adjusts individual predictions toward the mean.   (It applies to the second term in the rescaling shown above.)  The combination of the macro adjustment and the micro humility factor is equivalent to scaling down and adding a constant, which is what the magic numbers used to do.\n",
    "\n",
    "There is also a macro humility factor. The macro humility factor adjusts the macro predictions by averaging in a \"naive\" macro model. That gives enough degrees of freedom to get back the effect of the magic numbers, but the new magic numbers will be humility parameters instead of just numbers that make the public leaderboard score higher. Of course I will adjust the humility parameters to maximize the public leaderboard score, but now with a stronger theoretical justification. (This amounts to using the public test cases as a cross-validation set, which is not unreasonable.)\n",
    "\n",
    "Also, setting humility parameters, while having an *actual* macro model, means that the overall model can be used to predict the future beyond May 2016. Because there's a macro model, that model would be re-fit and used to predict future means, and the same humility parameters (or new ones, if one decides to change them based on private leaderboard performance) can be applied. You'll get actual predictions instead of the nonsense you would get by applying an arbitrary linear transformation that just happened to work well for the original test period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "micro_humility_factor = .99     #    range from 0 (complete humility) to 1 (no humility)\n",
    "macro_humility_factor = 0.96\n",
    "jason_weight = .2\n",
    "bruno_weight = .2\n",
    "reynaldo_weight = 1 - jason_weight - bruno_weight\n",
    "seed = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get ready for lots of annoying deprecation warnings\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import model_selection, preprocessing\n",
    "import xgboost as xgb\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Fit macro model and compute average prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "macro = pd.read_csv('./input/macro.csv')\n",
    "train = pd.read_csv('./input/train.csv')\n",
    "test = pd.read_csv('./input/test.csv')\n",
    "\n",
    "# Macro data monthly medians\n",
    "macro[\"timestamp\"] = pd.to_datetime(macro[\"timestamp\"])\n",
    "macro[\"year\"]  = macro[\"timestamp\"].dt.year\n",
    "macro[\"month\"] = macro[\"timestamp\"].dt.month\n",
    "macro[\"yearmonth\"] = 100*macro.year + macro.month\n",
    "macmeds = macro.groupby(\"yearmonth\").median()\n",
    "\n",
    "# Price data monthly medians\n",
    "train[\"timestamp\"] = pd.to_datetime(train[\"timestamp\"])\n",
    "train[\"year\"]  = train[\"timestamp\"].dt.year\n",
    "train[\"month\"] = train[\"timestamp\"].dt.month\n",
    "train[\"yearmonth\"] = 100*train.year + train.month\n",
    "prices = train[[\"yearmonth\",\"price_doc\"]]\n",
    "p = prices.groupby(\"yearmonth\").median()\n",
    "\n",
    "# Join monthly prices to macro data\n",
    "df = macmeds.join(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Function to process Almon lags\n",
    "\n",
    "import numpy.matlib as ml\n",
    " \n",
    "def almonZmatrix(X, maxlag, maxdeg):\n",
    "    \"\"\"\n",
    "    Creates the Z matrix corresponding to vector X.\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    Z = ml.zeros((len(X)-maxlag, maxdeg+1))\n",
    "    for t in range(maxlag,  n):\n",
    "       #Solve for Z[t][0].\n",
    "       Z[t-maxlag,0] = sum([X[t-lag] for lag in range(maxlag+1)])\n",
    "       for j in range(1, maxdeg+1):\n",
    "             s = 0.0\n",
    "             for i in range(1, maxlag+1):       \n",
    "                s += (i)**j * X[t-i]\n",
    "             Z[t-maxlag,j] = s\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6593788.9177278513"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data for macro model\n",
    "y = df.price_doc.div(df.cpi).apply(np.log).loc[201108:201506]\n",
    "lncpi = df.cpi.apply(np.log)\n",
    "tblags = 5    # Number of lags used on PDL for Trade Balance\n",
    "mrlags = 5    # Number of lags used on PDL for Mortgage Rate\n",
    "cplags = 5    # Number of lags used on PDL for CPI\n",
    "ztb = almonZmatrix(df.balance_trade.loc[201103:201506].as_matrix(), tblags, 1)\n",
    "zmr = almonZmatrix(df.mortgage_rate.loc[201103:201506].as_matrix(), mrlags, 1)\n",
    "zcp = almonZmatrix(lncpi.loc[201103:201506].as_matrix(), cplags, 1)\n",
    "columns = ['tb0', 'tb1', 'mr0', 'mr1', 'cp0', 'cp1']\n",
    "z = pd.DataFrame( np.concatenate( (ztb, zmr, zcp), axis=1), y.index.values, columns )\n",
    "X = sm.add_constant( z )\n",
    "\n",
    "# Fit macro model\n",
    "eq = sm.OLS(y, X)\n",
    "fit = eq.fit()\n",
    "\n",
    "# Predict with macro model\n",
    "test_cpi = df.cpi.loc[201507:201605]\n",
    "test_index = test_cpi.index\n",
    "ztb_test = almonZmatrix(df.balance_trade.loc[201502:201605].as_matrix(), tblags, 1)\n",
    "zmr_test = almonZmatrix(df.mortgage_rate.loc[201502:201605].as_matrix(), mrlags, 1)\n",
    "zcp_test = almonZmatrix(lncpi.loc[201502:201605].as_matrix(), cplags, 1)\n",
    "z_test = pd.DataFrame( np.concatenate( (ztb_test, zmr_test, zcp_test), axis=1), \n",
    "                       test_index, columns )\n",
    "X_test = sm.add_constant( z_test )\n",
    "pred_lnrp = fit.predict( X_test )\n",
    "pred_p = np.exp(pred_lnrp) * test_cpi\n",
    "\n",
    "# Merge with test cases and compute mean for macro prediction\n",
    "test[\"timestamp\"] = pd.to_datetime(test[\"timestamp\"])\n",
    "test[\"year\"]  = test[\"timestamp\"].dt.year\n",
    "test[\"month\"] = test[\"timestamp\"].dt.month\n",
    "test[\"yearmonth\"] = 100*test.year + test.month\n",
    "test_ids = test[[\"yearmonth\",\"id\"]]\n",
    "monthprices = pd.DataFrame({\"yearmonth\":pred_p.index.values,\"monthprice\":pred_p.values})\n",
    "macro_mean = np.exp(test_ids.merge(monthprices, on=\"yearmonth\").monthprice.apply(np.log).mean())\n",
    "macro_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7773440.7512406502"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive macro model assumes housing prices will simply follow CPI\n",
    "naive_pred_lnrp = y.mean()\n",
    "naive_pred_p = np.exp(naive_pred_lnrp) * test_cpi\n",
    "monthnaive = pd.DataFrame({\"yearmonth\":pred_p.index.values, \"monthprice\":naive_pred_p.values})\n",
    "macro_naive = np.exp(test_ids.merge(monthnaive, on=\"yearmonth\").monthprice.apply(np.log).mean())\n",
    "macro_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6637341.6089051161"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine naive and substantive macro models\n",
    "macro_mean = macro_naive * (macro_mean/macro_naive) ** macro_humility_factor\n",
    "macro_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Fit Jason's model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:13: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:15: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30474</td>\n",
       "      <td>5632583.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30475</td>\n",
       "      <td>8103520.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30476</td>\n",
       "      <td>5599423.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30477</td>\n",
       "      <td>5845963.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30478</td>\n",
       "      <td>4970532.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  price_doc\n",
       "0  30474  5632583.0\n",
       "1  30475  8103520.5\n",
       "2  30476  5599423.5\n",
       "3  30477  5845963.0\n",
       "4  30478  4970532.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Jason/Gunja\n",
    "\n",
    "\n",
    "\n",
    "#load files\n",
    "train = pd.read_csv('./input/train.csv', parse_dates=['timestamp'])\n",
    "test = pd.read_csv('./input/test.csv', parse_dates=['timestamp'])\n",
    "macro = pd.read_csv('./input/macro.csv', parse_dates=['timestamp'])\n",
    "id_test = test.id\n",
    "\n",
    "#clean data\n",
    "bad_index = train[train.life_sq > train.full_sq].index\n",
    "train.ix[bad_index, \"life_sq\"] = np.NaN\n",
    "equal_index = [601,1896,2791]\n",
    "test.ix[equal_index, \"life_sq\"] = test.ix[equal_index, \"full_sq\"]\n",
    "bad_index = test[test.life_sq > test.full_sq].index\n",
    "test.ix[bad_index, \"life_sq\"] = np.NaN\n",
    "bad_index = train[train.life_sq < 5].index\n",
    "train.ix[bad_index, \"life_sq\"] = np.NaN\n",
    "bad_index = test[test.life_sq < 5].index\n",
    "test.ix[bad_index, \"life_sq\"] = np.NaN\n",
    "bad_index = train[train.full_sq < 5].index\n",
    "train.ix[bad_index, \"full_sq\"] = np.NaN\n",
    "bad_index = test[test.full_sq < 5].index\n",
    "test.ix[bad_index, \"full_sq\"] = np.NaN\n",
    "kitch_is_build_year = [13117]\n",
    "train.ix[kitch_is_build_year, \"build_year\"] = train.ix[kitch_is_build_year, \"kitch_sq\"]\n",
    "bad_index = train[train.kitch_sq >= train.life_sq].index\n",
    "train.ix[bad_index, \"kitch_sq\"] = np.NaN\n",
    "bad_index = test[test.kitch_sq >= test.life_sq].index\n",
    "test.ix[bad_index, \"kitch_sq\"] = np.NaN\n",
    "bad_index = train[(train.kitch_sq == 0).values + (train.kitch_sq == 1).values].index\n",
    "train.ix[bad_index, \"kitch_sq\"] = np.NaN\n",
    "bad_index = test[(test.kitch_sq == 0).values + (test.kitch_sq == 1).values].index\n",
    "test.ix[bad_index, \"kitch_sq\"] = np.NaN\n",
    "bad_index = train[(train.full_sq > 210) & (train.life_sq / train.full_sq < 0.3)].index\n",
    "train.ix[bad_index, \"full_sq\"] = np.NaN\n",
    "bad_index = test[(test.full_sq > 150) & (test.life_sq / test.full_sq < 0.3)].index\n",
    "test.ix[bad_index, \"full_sq\"] = np.NaN\n",
    "bad_index = train[train.life_sq > 300].index\n",
    "train.ix[bad_index, [\"life_sq\", \"full_sq\"]] = np.NaN\n",
    "bad_index = test[test.life_sq > 200].index\n",
    "test.ix[bad_index, [\"life_sq\", \"full_sq\"]] = np.NaN\n",
    "train.product_type.value_counts(normalize= True)\n",
    "test.product_type.value_counts(normalize= True)\n",
    "bad_index = train[train.build_year < 1500].index\n",
    "train.ix[bad_index, \"build_year\"] = np.NaN\n",
    "bad_index = test[test.build_year < 1500].index\n",
    "test.ix[bad_index, \"build_year\"] = np.NaN\n",
    "bad_index = train[train.num_room == 0].index \n",
    "train.ix[bad_index, \"num_room\"] = np.NaN\n",
    "bad_index = test[test.num_room == 0].index \n",
    "test.ix[bad_index, \"num_room\"] = np.NaN\n",
    "bad_index = [10076, 11621, 17764, 19390, 24007, 26713, 29172]\n",
    "train.ix[bad_index, \"num_room\"] = np.NaN\n",
    "bad_index = [3174, 7313]\n",
    "test.ix[bad_index, \"num_room\"] = np.NaN\n",
    "bad_index = train[(train.floor == 0).values * (train.max_floor == 0).values].index\n",
    "train.ix[bad_index, [\"max_floor\", \"floor\"]] = np.NaN\n",
    "bad_index = train[train.floor == 0].index\n",
    "train.ix[bad_index, \"floor\"] = np.NaN\n",
    "bad_index = train[train.max_floor == 0].index\n",
    "train.ix[bad_index, \"max_floor\"] = np.NaN\n",
    "bad_index = test[test.max_floor == 0].index\n",
    "test.ix[bad_index, \"max_floor\"] = np.NaN\n",
    "bad_index = train[train.floor > train.max_floor].index\n",
    "train.ix[bad_index, \"max_floor\"] = np.NaN\n",
    "bad_index = test[test.floor > test.max_floor].index\n",
    "test.ix[bad_index, \"max_floor\"] = np.NaN\n",
    "train.floor.describe(percentiles= [0.9999])\n",
    "bad_index = [23584]\n",
    "train.ix[bad_index, \"floor\"] = np.NaN\n",
    "train.material.value_counts()\n",
    "test.material.value_counts()\n",
    "train.state.value_counts()\n",
    "bad_index = train[train.state == 33].index\n",
    "train.ix[bad_index, \"state\"] = np.NaN\n",
    "test.state.value_counts()\n",
    "\n",
    "# brings error down a lot by removing extreme price per sqm\n",
    "train.loc[train.full_sq == 0, 'full_sq'] = 50\n",
    "train = train[train.price_doc/train.full_sq <= 600000]\n",
    "train = train[train.price_doc/train.full_sq >= 10000]\n",
    "\n",
    "# Add month-year\n",
    "month_year = (train.timestamp.dt.month + train.timestamp.dt.year * 100)\n",
    "month_year_cnt_map = month_year.value_counts().to_dict()\n",
    "train['month_year_cnt'] = month_year.map(month_year_cnt_map)\n",
    "\n",
    "month_year = (test.timestamp.dt.month + test.timestamp.dt.year * 100)\n",
    "month_year_cnt_map = month_year.value_counts().to_dict()\n",
    "test['month_year_cnt'] = month_year.map(month_year_cnt_map)\n",
    "\n",
    "# Add week-year count\n",
    "week_year = (train.timestamp.dt.weekofyear + train.timestamp.dt.year * 100)\n",
    "week_year_cnt_map = week_year.value_counts().to_dict()\n",
    "train['week_year_cnt'] = week_year.map(week_year_cnt_map)\n",
    "\n",
    "week_year = (test.timestamp.dt.weekofyear + test.timestamp.dt.year * 100)\n",
    "week_year_cnt_map = week_year.value_counts().to_dict()\n",
    "test['week_year_cnt'] = week_year.map(week_year_cnt_map)\n",
    "\n",
    "# Add month and day-of-week\n",
    "train['month'] = train.timestamp.dt.month\n",
    "train['dow'] = train.timestamp.dt.dayofweek\n",
    "\n",
    "test['month'] = test.timestamp.dt.month\n",
    "test['dow'] = test.timestamp.dt.dayofweek\n",
    "\n",
    "# Other feature engineering\n",
    "train['rel_floor'] = train['floor'] / train['max_floor'].astype(float)\n",
    "train['rel_kitch_sq'] = train['kitch_sq'] / train['full_sq'].astype(float)\n",
    "\n",
    "test['rel_floor'] = test['floor'] / test['max_floor'].astype(float)\n",
    "test['rel_kitch_sq'] = test['kitch_sq'] / test['full_sq'].astype(float)\n",
    "\n",
    "train.apartment_name=train.sub_area + train['metro_km_avto'].astype(str)\n",
    "test.apartment_name=test.sub_area + train['metro_km_avto'].astype(str)\n",
    "\n",
    "train['room_size'] = train['life_sq'] / train['num_room'].astype(float)\n",
    "test['room_size'] = test['life_sq'] / test['num_room'].astype(float)\n",
    "\n",
    "y_train = train[\"price_doc\"]\n",
    "wts = 1 - .47*(y_train == 1e6)\n",
    "x_train = train.drop([\"id\", \"timestamp\", \"price_doc\"], axis=1)\n",
    "x_test = test.drop([\"id\", \"timestamp\"], axis=1)\n",
    "\n",
    "for c in x_train.columns:\n",
    "    if x_train[c].dtype == 'object':\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(x_train[c].values)) \n",
    "        x_train[c] = lbl.transform(list(x_train[c].values))\n",
    "        #x_train.drop(c,axis=1,inplace=True)\n",
    "        \n",
    "for c in x_test.columns:\n",
    "    if x_test[c].dtype == 'object':\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(x_test[c].values)) \n",
    "        x_test[c] = lbl.transform(list(x_test[c].values))\n",
    "        #x_test.drop(c,axis=1,inplace=True)  \n",
    "\n",
    "\n",
    "xgb_params = {\n",
    "    'eta': 0.05,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'silent': 1,\n",
    "    'seed': seed\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(x_train, y_train, weight=wts)\n",
    "dtest = xgb.DMatrix(x_test)\n",
    "\n",
    "#cv_output = xgb.cv(xgb_params, dtrain, num_boost_round=1000, early_stopping_rounds=20,\n",
    "#    verbose_eval=50, show_stdv=False)\n",
    "#cv_output[['train-rmse-mean', 'test-rmse-mean']].plot()\n",
    "\n",
    "#num_boost_rounds = len(cv_output)\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=350)\n",
    "\n",
    "#fig, ax = plt.subplots(1, 1, figsize=(8, 13))\n",
    "#xgb.plot_importance(model, max_num_features=50, height=0.5, ax=ax)\n",
    "\n",
    "y_predict = model.predict(dtest)\n",
    "jason_model_output = pd.DataFrame({'id': id_test, 'price_doc': y_predict})\n",
    "jason_model_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6848545.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jason_model_output.to_csv('jason_model.csv', index=False)\n",
    "np.exp( jason_model_output.price_doc.apply(np.log).mean() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Fit Reynaldo's model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30474</td>\n",
       "      <td>5548605.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30475</td>\n",
       "      <td>8466520.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30476</td>\n",
       "      <td>5481566.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30477</td>\n",
       "      <td>5629051.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30478</td>\n",
       "      <td>5264548.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  price_doc\n",
       "0  30474  5548605.0\n",
       "1  30475  8466520.0\n",
       "2  30476  5481566.5\n",
       "3  30477  5629051.0\n",
       "4  30478  5264548.5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reynaldo\n",
    "\n",
    "\n",
    "train = pd.read_csv('./input/train.csv')\n",
    "test = pd.read_csv('./input/test.csv')\n",
    "id_test = test.id\n",
    "\n",
    "y_train = train[\"price_doc\"]\n",
    "x_train = train.drop([\"id\", \"timestamp\", \"price_doc\"], axis=1)\n",
    "x_test = test.drop([\"id\", \"timestamp\"], axis=1)\n",
    "\n",
    "for c in x_train.columns:\n",
    "    if x_train[c].dtype == 'object':\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(x_train[c].values)) \n",
    "        x_train[c] = lbl.transform(list(x_train[c].values))\n",
    "        \n",
    "for c in x_test.columns:\n",
    "    if x_test[c].dtype == 'object':\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(x_test[c].values)) \n",
    "        x_test[c] = lbl.transform(list(x_test[c].values))\n",
    "\n",
    "xgb_params = {\n",
    "    'eta': 0.05,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'silent': 1,\n",
    "    'seed': seed\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(x_train, y_train)\n",
    "dtest = xgb.DMatrix(x_test)\n",
    "\n",
    "num_boost_rounds = 384  # This was the CV output, as earlier version shows\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round= num_boost_rounds)\n",
    "\n",
    "y_predict = model.predict(dtest)\n",
    "reynaldo_model_output = pd.DataFrame({'id': id_test, 'price_doc': y_predict})\n",
    "reynaldo_model_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6867420.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reynaldo_model_output.to_csv('reynaldo_model.csv', index=False)\n",
    "np.exp( reynaldo_model_output.price_doc.apply(np.log).mean() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Fit Bruno's model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38132, 390)\n",
      "(38132, 394)\n",
      "(38132, 394)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30474</td>\n",
       "      <td>5522187.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30475</td>\n",
       "      <td>8077244.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30476</td>\n",
       "      <td>5338578.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30477</td>\n",
       "      <td>5637760.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30478</td>\n",
       "      <td>5038952.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  price_doc\n",
       "0  30474  5522187.0\n",
       "1  30475  8077244.0\n",
       "2  30476  5338578.0\n",
       "3  30477  5637760.0\n",
       "4  30478  5038952.5"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bruno with outlier dropped\n",
    "\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "df_train = pd.read_csv(\"./input/train.csv\", parse_dates=['timestamp'])\n",
    "df_test = pd.read_csv(\"./input/test.csv\", parse_dates=['timestamp'])\n",
    "df_macro = pd.read_csv(\"./input/macro.csv\", parse_dates=['timestamp'])\n",
    "\n",
    "df_train.drop(df_train[df_train[\"life_sq\"] > 7000].index, inplace=True)\n",
    "\n",
    "y_train = df_train['price_doc'].values\n",
    "id_test = df_test['id']\n",
    "\n",
    "df_train.drop(['id', 'price_doc'], axis=1, inplace=True)\n",
    "df_test.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "num_train = len(df_train)\n",
    "df_all = pd.concat([df_train, df_test])\n",
    "# Next line just adds a lot of NA columns (becuase \"join\" only works on indexes)\n",
    "# but somewhow it seems to affect the result\n",
    "df_all = df_all.join(df_macro, on='timestamp', rsuffix='_macro')\n",
    "print(df_all.shape)\n",
    "\n",
    "# Add month-year\n",
    "month_year = (df_all.timestamp.dt.month + df_all.timestamp.dt.year * 100)\n",
    "month_year_cnt_map = month_year.value_counts().to_dict()\n",
    "df_all['month_year_cnt'] = month_year.map(month_year_cnt_map)\n",
    "\n",
    "# Add week-year count\n",
    "week_year = (df_all.timestamp.dt.weekofyear + df_all.timestamp.dt.year * 100)\n",
    "week_year_cnt_map = week_year.value_counts().to_dict()\n",
    "df_all['week_year_cnt'] = week_year.map(week_year_cnt_map)\n",
    "\n",
    "# Add month and day-of-week\n",
    "df_all['month'] = df_all.timestamp.dt.month\n",
    "df_all['dow'] = df_all.timestamp.dt.dayofweek\n",
    "\n",
    "# Other feature engineering\n",
    "df_all['rel_floor'] = df_all['floor'] / df_all['max_floor'].astype(float)\n",
    "df_all['rel_kitch_sq'] = df_all['kitch_sq'] / df_all['full_sq'].astype(float)\n",
    "\n",
    "# Remove timestamp column (may overfit the model in train)\n",
    "df_all.drop(['timestamp', 'timestamp_macro'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "factorize = lambda t: pd.factorize(t[1])[0]\n",
    "\n",
    "df_obj = df_all.select_dtypes(include=['object'])\n",
    "\n",
    "X_all = np.c_[\n",
    "    df_all.select_dtypes(exclude=['object']).values,\n",
    "    np.array(list(map(factorize, df_obj.iteritems()))).T\n",
    "]\n",
    "print(X_all.shape)\n",
    "\n",
    "X_train = X_all[:num_train]\n",
    "X_test = X_all[num_train:]\n",
    "\n",
    "\n",
    "# Deal with categorical values\n",
    "df_numeric = df_all.select_dtypes(exclude=['object'])\n",
    "df_obj = df_all.select_dtypes(include=['object']).copy()\n",
    "\n",
    "for c in df_obj:\n",
    "    df_obj[c] = pd.factorize(df_obj[c])[0]\n",
    "\n",
    "df_values = pd.concat([df_numeric, df_obj], axis=1)\n",
    "\n",
    "\n",
    "# Convert to numpy values\n",
    "X_all = df_values.values\n",
    "print(X_all.shape)\n",
    "\n",
    "X_train = X_all[:num_train]\n",
    "X_test = X_all[num_train:]\n",
    "\n",
    "df_columns = df_values.columns\n",
    "\n",
    "\n",
    "xgb_params = {\n",
    "    'eta': 0.05,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'silent': 1,\n",
    "    'seed': seed\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, y_train, feature_names=df_columns)\n",
    "dtest = xgb.DMatrix(X_test, feature_names=df_columns)\n",
    "\n",
    "\n",
    "num_boost_round = 489  # From Bruno's original CV, I think\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_round)\n",
    "\n",
    "y_pred = model.predict(dtest)\n",
    "bruno_model_output = pd.DataFrame({'id': id_test, 'price_doc': y_pred})\n",
    "bruno_model_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6774162.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bruno_model_output.to_csv('bruno_model.csv', index=False)\n",
    "np.exp( bruno_model_output.price_doc.apply(np.log).mean() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Merge and adjust the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30474</td>\n",
       "      <td>5560001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30475</td>\n",
       "      <td>8314010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30476</td>\n",
       "      <td>5475916.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30477</td>\n",
       "      <td>5673535.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30478</td>\n",
       "      <td>5159003.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  price_doc\n",
       "0  30474  5560001.0\n",
       "1  30475  8314010.0\n",
       "2  30476  5475916.0\n",
       "3  30477  5673535.0\n",
       "4  30478  5159003.5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge\n",
    "\n",
    "results = reynaldo_model_output.merge( \n",
    "             jason_model_output.merge(\n",
    "                 bruno_model_output, on='id', suffixes=['_jason','_bruno'] ), on='id' )\n",
    "results[\"price_doc_reynaldo\"] = results[\"price_doc\"]\n",
    "results[\"price_doc\"] = np.exp( np.log(results.price_doc_reynaldo)*reynaldo_weight +\n",
    "                               np.log(results.price_doc_jason)*jason_weight       +\n",
    "                               np.log(results.price_doc_bruno)*bruno_weight          )\n",
    "\n",
    "results.drop([\"price_doc_reynaldo\", \"price_doc_bruno\", \"price_doc_jason\"],axis=1,inplace=True)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "results.to_csv('unadjusted_combo.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30474</td>\n",
       "      <td>5368491.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30475</td>\n",
       "      <td>8094075.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30476</td>\n",
       "      <td>5285654.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30477</td>\n",
       "      <td>5480382.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30478</td>\n",
       "      <td>4973673.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  price_doc\n",
       "0  30474  5368491.5\n",
       "1  30475  8094075.5\n",
       "2  30476  5285654.0\n",
       "3  30477  5480382.5\n",
       "4  30478  4973673.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjust\n",
    "\n",
    "lny = np.log(results.price_doc)\n",
    "lnm = np.log(macro_mean)\n",
    "\n",
    "# I'm not sure whether this makes any sense or not.\n",
    "# 1+lny.mean()-lnm term is meant to offest the scale effect of the logarithmic mean shift\n",
    "#   while allowing the new logarithmic mean to remain at lnm.\n",
    "y_trans = lnm  +  micro_humility_factor * (lny-lny.mean()) * (1+lny.mean()-lnm)\n",
    "y_predict = np.exp( y_trans )\n",
    "\n",
    "sub = pd.DataFrame({'id': id_test, 'price_doc': y_predict})\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sub.to_csv('submission_jun7_5_Seed5_microHumanilityPt99_Latest_Iteration_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
