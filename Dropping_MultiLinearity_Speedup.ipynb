{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Forked from https://www.kaggle.com/ffisegydd/sberbank-russian-housing-market/sklearn-multicollinearity-class by Ffisegydd. As noted by original author the script is very slow. This one provided a significant speedup (when applied to many columns with infinite VIF values as is the case here) by not adding much code.\n",
    "\n",
    "This notebook is based on the prior work done by Roberto Ruiz in https://www.kaggle.com/robertoruiz/sberbank-russian-housing-market/dealing-with-multicollinearity. As such, I won't go into explanation of the theory because they have already done a fantastic job of doing so. If you have any questions on the theory then please feel free to ask, but I'd definitely suggest that everyone read Roberto's work as an introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "# Extra imports necessary for the code\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Based on the work set out by Roberto, I've created a scikit-learn transformer class that can be used to remove columns that have a high VIF factor (in short, they have high colinearity with other columns within the dataset and as such should probably be removed).\n",
    "\n",
    "This class is based on a standard scikit-learn transformer and also uses the statsmodel library for calculating the VIF number. Information on scikit-learn transformers can be found [here](http://scikit-learn.org/stable/data_transforms.html) whilst the docs for the statsmodel function can be found [here](http://www.statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv('./input/X_train_cleaned.csv', index_col=0)#Initial_XGB\n",
    "y = pd.read_csv('./input/train.csv').pop('price_doc')\n",
    "#X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "class ReduceVIF(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, thresh=5.0, impute=True, impute_strategy='median'):\n",
    "        # From looking at documentation, values between 5 and 10 are \"okay\".\n",
    "        # Above 10 is too high and so should be removed.\n",
    "        self.thresh = thresh\n",
    "        \n",
    "        # The statsmodel function will fail with NaN values, as such we have to impute them.\n",
    "        # By default we impute using the median value.\n",
    "        # This imputation could be taken out and added as part of an sklearn Pipeline.\n",
    "        if impute:\n",
    "            self.imputer = Imputer(strategy=impute_strategy)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print('ReduceVIF fit')\n",
    "        if hasattr(self, 'imputer'):\n",
    "            self.imputer.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print('ReduceVIF transform')\n",
    "        columns = X.columns.tolist()\n",
    "        if hasattr(self, 'imputer'):\n",
    "            X = pd.DataFrame(self.imputer.transform(X), columns=columns)\n",
    "        return ReduceVIF.calculate_vif(X, self.thresh)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_vif(X, thresh=5.0):\n",
    "        # Taken from https://stats.stackexchange.com/a/253620/53565 and modified\n",
    "        dropped=True\n",
    "        while dropped:\n",
    "            # Loop repeatedly until we find that all columns within our dataset\n",
    "            # have a VIF value we're happy with.\n",
    "            variables = X.columns\n",
    "            dropped=False\n",
    "            vif = []\n",
    "            new_vif = 0\n",
    "            for var in X.columns:\n",
    "                new_vif = variance_inflation_factor(X[variables].values, X.columns.get_loc(var))\n",
    "                vif.append(new_vif)\n",
    "                if np.isinf(new_vif):\n",
    "                    break\n",
    "            max_vif = max(vif)\n",
    "            if max_vif > thresh:\n",
    "                maxloc = vif.index(max_vif)\n",
    "                print('Dropping '+ str(X.columns[maxloc])+ ' with vif = '+str(max_vif))\n",
    "                X = X.drop([X.columns.tolist()[maxloc]], axis=1)\n",
    "                dropped=True\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReduceVIF fit\n",
      "ReduceVIF transform\n",
      "Dropping cafe_count_3000 with vif = inf\n",
      "Dropping cafe_count_5000 with vif = inf\n",
      "Dropping cafe_avg_price_5000 with vif = 4255761122.63\n",
      "Dropping cafe_avg_price_3000 with vif = 1069127245.49\n",
      "Dropping cafe_sum_5000_max_price_avg with vif = 7673.95207771\n",
      "Dropping cafe_sum_3000_max_price_avg with vif = 1665.45841511\n",
      "Dropping cafe_count_5000_price_1000 with vif = 1209.61406718\n",
      "length of X.columns43\n",
      "Index([u'trc_count_3000', u'trc_sqm_3000', u'cafe_sum_3000_min_price_avg',\n",
      "       u'cafe_count_3000_na_price', u'cafe_count_3000_price_500',\n",
      "       u'cafe_count_3000_price_1000', u'cafe_count_3000_price_1500',\n",
      "       u'cafe_count_3000_price_2500', u'cafe_count_3000_price_4000',\n",
      "       u'cafe_count_3000_price_high', u'big_church_count_3000',\n",
      "       u'church_count_3000', u'mosque_count_3000', u'leisure_count_3000',\n",
      "       u'sport_count_3000', u'market_count_3000', u'green_part_5000',\n",
      "       u'prom_part_5000', u'office_count_5000', u'office_sqm_5000',\n",
      "       u'trc_count_5000', u'trc_sqm_5000', u'cafe_sum_5000_min_price_avg',\n",
      "       u'cafe_count_5000_na_price', u'cafe_count_5000_price_500',\n",
      "       u'cafe_count_5000_price_1500', u'cafe_count_5000_price_2500',\n",
      "       u'cafe_count_5000_price_4000', u'cafe_count_5000_price_high',\n",
      "       u'big_church_count_5000', u'church_count_5000', u'mosque_count_5000',\n",
      "       u'leisure_count_5000', u'sport_count_5000', u'market_count_5000',\n",
      "       u'month_year_cnt', u'week_year_cnt', u'year', u'month', u'dow',\n",
      "       u'rel_floor', u'rel_kitch_sq', u'room_size'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "transformer = ReduceVIF(thresh=1000.0)\n",
    "\n",
    "# Only use 10 columns for speed in this example\n",
    "X = transformer.fit_transform(X[X.columns[-50:]], y)\n",
    "\n",
    "print(\"\\nlength of X.columns\"+str(len(X.columns))+\"\\n\")\n",
    "\n",
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Things to bear in mind:\n",
    "\n",
    " - It's really slow, especially at the beginning when it's calculating VIF for all columns. As it drops more columns it will speed up, but there's a reason I've only used 10 columns for the example...\n",
    " - If two or more columns \"tie\" (in the case when multiple can be inf...) then it'll drop the first column it finds to start with.\n",
    " - The idea of imputing so much data makes me uneasy. I'd be tempted to modify it so you use the imputed data only for dropping columns but then continue analysis with all of the NaNs present."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
