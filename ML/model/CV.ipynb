{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "使用基本的特征构建基本的 xgboost 模型\n",
    "@author: MarkLiu\n",
    "@time  : 17-5-25 下午9:03\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "sys.path.append(module_path)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "# remove warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "from readData import readData, data_preprocess\n",
    "\n",
    "# my own module\n",
    "from features import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conbined_data: (36650, 702)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train = readData(isLog1p=True)\n",
    "#X_train, X_test = data_preprocess(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def CV(train, y, model, test_size=.2, isLog1p=True):\n",
    "    X_train, X_val, y_train, y_val=train_test_split(train, y, test_size=test_size, \n",
    "                                                random_state=123)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    if (isLog1p):\n",
    "        train_rmse = mean_squared_error(y_train, model.predict(X_train))\n",
    "        val_rmse = mean_squared_error(y_val, model.predict(X_val))\n",
    "    else:\n",
    "        train_rmse = mean_squared_error(np.log1p(y_train), np.log1p(model.predict(X_train)))\n",
    "        val_rmse = mean_squared_error(np.log1p(y_val), np.log1p(model.predict(X_val)))\n",
    "    print 'train_rmse =', np.sqrt(train_rmse), ', val_rmse =', np.sqrt(val_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:14.4027\ttest-rmse:14.4028\n",
      "[50]\ttrain-rmse:1.15845\ttest-rmse:1.1601\n",
      "[100]\ttrain-rmse:0.324909\ttest-rmse:0.33664\n",
      "[150]\ttrain-rmse:0.303985\ttest-rmse:0.321419\n",
      "[200]\ttrain-rmse:0.297296\ttest-rmse:0.319335\n",
      "[250]\ttrain-rmse:0.291982\ttest-rmse:0.318278\n",
      "[300]\ttrain-rmse:0.287582\ttest-rmse:0.317583\n",
      "[350]\ttrain-rmse:0.283497\ttest-rmse:0.317135\n",
      "[400]\ttrain-rmse:0.279629\ttest-rmse:0.316825\n",
      "[450]\ttrain-rmse:0.275974\ttest-rmse:0.316666\n",
      "[500]\ttrain-rmse:0.272538\ttest-rmse:0.316542\n",
      "[550]\ttrain-rmse:0.269331\ttest-rmse:0.316507\n",
      "[600]\ttrain-rmse:0.266131\ttest-rmse:0.316368\n",
      "[650]\ttrain-rmse:0.263164\ttest-rmse:0.316343\n",
      "620\n"
     ]
    }
   ],
   "source": [
    "# xgb_params = {\n",
    "#         'learning_rate': 0.05,\n",
    "#         'max_depth': 5,\n",
    "#         'subsample': 0.7,\n",
    "#         'colsample_bytree': 0.7,\n",
    "#         'objective': 'reg:linear',\n",
    "#         #'eval_metric': 'rmse',\n",
    "#         'min_child_weight': 1,\n",
    "#         'silent': 1,\n",
    "#         'seed':5\n",
    "#     }\n",
    "\n",
    "xgb_params = {'learning_rate': 0.05,\n",
    "              'max_depth': 4,\n",
    "              'subsample': 0.95,\n",
    "              'reg_alpha': 0,\n",
    "              'min_child_weight':4,\n",
    "              'colsample_bytree': 0.95,\n",
    "              'gamma':.4,\n",
    "              'objective': 'reg:linear',\n",
    "              'silent': 1,\n",
    "              #'booster' :'gbtree',\n",
    "              #'tuneLength': 3,\n",
    "              'seed': 5 }\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "cv_output = xgb.cv(xgb_params, dtrain, num_boost_round=1000, early_stopping_rounds=40, nfold=5,\\\n",
    "    verbose_eval=50, show_stdv=False)\n",
    "cv_output[['train-rmse-mean', 'test-rmse-mean']].plot()\n",
    "\n",
    "print(len(cv_output)) #309\n",
    "\n",
    "xgb_params['n_estimators']=len(cv_output)\n",
    "\n",
    "model = XGBRegressor(**xgb_params)\n",
    "\n",
    "# model = XGBRegressor(seed=5, learning_rate=.05, subsample=.7, max_depth=5, #min_child_weight=3, \n",
    "#             n_estimators=281, \n",
    "#             colsample_bytree=0.7\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "xgb_params = {'learning_rate': 0.05,\n",
    "              'max_depth': 4,\n",
    "              'subsample': 0.95,\n",
    "              'reg_alpha': 0,\n",
    "              'min_child_weight':4,\n",
    "              'colsample_bytree': 0.95,\n",
    "              'gamma':.4,\n",
    "              'objective': 'reg:linear',\n",
    "              'silent': 1,\n",
    "              #'booster' :'gbtree',\n",
    "              #'tuneLength': 3,\n",
    "              'seed': 5 }\n",
    "#print(len(cv_output)) #309\n",
    "\n",
    "xgb_params['n_estimators']=620 #len(cv_output)\n",
    "\n",
    "model = XGBRegressor(**xgb_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'n_jobs': 16,\n",
    "    'n_estimators': 100,\n",
    "    'max_features': 0.2,\n",
    "    'max_depth': 12,\n",
    "    'min_samples_leaf': 2,\n",
    "}\n",
    "model1 = RandomForestRegressor(**rf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "et_params = {\n",
    "    'n_jobs': 16,\n",
    "    'n_estimators': 100,\n",
    "    'max_features': 0.5,\n",
    "    'max_depth': 12,\n",
    "    'min_samples_leaf': 2,\n",
    "    'random_state': 10\n",
    "}\n",
    "model2 = ExtraTreesRegressor(**et_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model3=GradientBoostingRegressor(learning_rate=0.02, n_estimators=500,min_samples_leaf=70, \n",
    "                                 min_samples_split=200, max_features='sqrt',max_depth=6,subsample=0.85,\n",
    "                                 random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb\n",
      "train_rmse = 0.266837631073 , val_rmse = 0.31900151058\n",
      "rf\n",
      "train_rmse = 0.241249700631 , val_rmse = 0.324802592639\n",
      "et\n",
      "train_rmse = 0.247191933346 , val_rmse = 0.324373969057\n",
      "gbm\n",
      "train_rmse = 0.290022115756 , val_rmse = 0.319881574992\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = data_preprocess(X_train, X_test)\n",
    "for m, name in zip([model, model1, model2, model3],['xgb','rf','et','gbm']):\n",
    "    print(name)\n",
    "    CV(X_train, y_train, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test = data_preprocess(X_train, X_test)\n",
    "rd_params = {'alpha': .5}\n",
    "model = Ridge(**rd_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls_params = {\n",
    "    'alpha': 0.005\n",
    "}\n",
    "model = Lasso(**ls_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_rmse = 0.334846893112 , val_rmse = 0.339948393494\n"
     ]
    }
   ],
   "source": [
    "CV(X_train, y_train, model)\n",
    "#not log1p, original train_rmse = 0.252746403304 , val_rmse = 0.320226748185\n",
    "#log1p, orginal train_rmse = 0.257648786803 , val_rmse = 0.320517933831, \n",
    "\n",
    "#log1p, my deleted train_rmse = 0.257648786803 , val_rmse = 0.320517933831, \n",
    "#log1p, my deleted added train_rmse = 0.257648786803 , val_rmse = 0.320517933831\n",
    "\n",
    "#log1p seed=5, learning_rate=.05, subsample=.7, max_depth=5, #min_child_weight=3, n_estimators=281, \n",
    "#colsample_bytree=0.7 train_rmse = 0.266050150088 , val_rmse = 0.319972286248\n",
    "\n",
    "#jun23 data log1p xgb seed=5, learning_rate=.05, subsample=.7, max_depth=5, #min_child_weight=3, n_estimators=309, \n",
    "#colsample_bytree=0.7 train_rmse = 0.261787007971 , val_rmse = 0.31980116091\n",
    "\n",
    "\n",
    "#jun24 data log1p xgb seed=5, learning_rate=.05, subsample=.7, max_depth=5, #min_child_weight=3, n_estimators=332, \n",
    "#colsample_bytree=0.7, train_rmse = 0.258649992867 , val_rmse = 0.320011541733\n",
    "\n",
    "#jun24 data log1p xgb_params = {'learning_rate': 0.05,'max_depth': 4,'subsample': 0.95,'reg_alpha': 0,\n",
    "#'min_child_weight':4,'colsample_bytree': 0.95,'gamma':.4,'objective': 'reg:linear',\n",
    "#'seed': 5, 'n_estimators':620 train_rmse = 0.265971694683 , val_rmse = 0.319114085918\n",
    "\n",
    "#jun24 data log1p GradientBoostingRegressor(learning_rate=0.02, n_estimators=500,min_samples_leaf=70, \n",
    "#min_samples_split=200, max_features='sqrt',max_depth=6,subsample=0.85,random_state=10)\n",
    "#train_rmse = 0.265971694683 , val_rmse = 0.319114085918\n",
    "\n",
    "#jun24 data log1p RF rf_params = {'n_jobs': 16,'n_estimators': 100,'max_features': 0.2,'max_depth': 12,\n",
    "#'min_samples_leaf': 2}train_rmse = 0.241249700631 , val_rmse = 0.324802592639\n",
    "\n",
    "#jun24 data log1p ET et_params = {'n_jobs': 16,'n_estimators': 100,'max_features': 0.5,'max_depth': 12,\n",
    "#'min_samples_leaf': 2,'random_state': 10} train_rmse = 0.247191933346 , val_rmse = 0.324373969057\n",
    "\n",
    "#jun25 data log1p xgb_params = {'learning_rate': 0.05,'max_depth': 4,'subsample': 0.95,'reg_alpha': 0,\n",
    "#'min_child_weight':4,'colsample_bytree': 0.95,'gamma':.4,'objective': 'reg:linear',\n",
    "#'seed': 5, 'n_estimators':620 train_rmse = 0.265971694683 , val_rmse = 0.319114085918\n",
    "\n",
    "#jun25 data log1p, rd alpha=10, train_rmse = 0.321207016244 , val_rmse = 0.33232610611\n",
    "#jun25 data log1p, rd alpha=1, 0.320498636946 , val_rmse = 0.332092993125\n",
    "\n",
    "#jun25 data log1p, ls alpha=.005, train_rmse = 0.334846893112 , val_rmse = 0.339948393494"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "isLog1p=True\n",
    "if (True):\n",
    "    train, test, macro = data_utils.load_data()\n",
    "\n",
    "    mult = .969\n",
    "\n",
    "    train['price_doc'] = train[\"price_doc\"] * mult + 10\n",
    "    if (isLog1p):\n",
    "        train['price_doc'] = np.log1p(train['price_doc'])\n",
    "    ylog_train_all = train['price_doc']\n",
    "    id_train = train['id']\n",
    "    train.drop(['id', 'price_doc'], axis=1, inplace=True)\n",
    "    #submit_ids = test['id']\n",
    "    submit_ids = pd.read_csv('../../input/test.csv')['id']\n",
    "    test.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "    # 合并训练集和测试集\n",
    "    conbined_data = pd.concat([train[test.columns.values], test])\n",
    "    # macro_cols = [\"balance_trade\", \"balance_trade_growth\", \"eurrub\", \"average_provision_of_build_contract\",\n",
    "    #               \"micex_rgbi_tr\", \"micex_cbi_tr\", \"deposits_rate\", \"mortgage_value\", \"mortgage_rate\",\n",
    "    #               \"income_per_cap\", \"rent_price_4+room_bus\", \"museum_visitis_per_100_cap\", \"apartment_build\", \"timestamp\"]\n",
    "    # conbined_data = pd.merge_ordered(conbined_data, macro[macro_cols], on='timestamp', how='left')\n",
    "\n",
    "    conbined_data.drop(['timestamp'], axis=1, inplace=True)\n",
    "    print \"conbined_data:\", conbined_data.shape\n",
    "\n",
    "    # Deal with categorical values\n",
    "    for c in conbined_data.columns:\n",
    "        if conbined_data[c].dtype == 'object':\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(conbined_data[c].values))\n",
    "            conbined_data[c] = lbl.transform(list(conbined_data[c].values))\n",
    "\n",
    "    train = conbined_data.iloc[:train.shape[0], :]\n",
    "    test = conbined_data.iloc[train.shape[0]:, :]\n",
    "\n",
    "    test_size = (1.0 * test.shape[0]) / train.shape[0]\n",
    "    print \"submit test size:\", test_size\n",
    "\n",
    "    # Convert to numpy values\n",
    "    X_all = train.values\n",
    "\n",
    "    # Create a validation set, with last 20% of data\n",
    "    num_train = int(train.shape[0] / (1+test_size))\n",
    "\n",
    "    X_train_all = X_all\n",
    "    X_test = test\n",
    "\n",
    "    # X_train = X_all[:num_train]\n",
    "    # X_val = X_all[num_train:]\n",
    "    # ylog_train = ylog_train_all[:num_train]\n",
    "    # ylog_val = ylog_train_all[num_train:]\n",
    "\n",
    "    X_train, X_val, ylog_train, ylog_val=train_test_split(X_all, ylog_train_all, test_size=test_size, \n",
    "                                                random_state=123)\n",
    "\n",
    "    print \"validate size:\", 1.0*X_val.shape[0] / X_train.shape[0]\n",
    "\n",
    "    df_columns = train.columns\n",
    "\n",
    "    print('X_train_all shape is', X_train_all.shape)\n",
    "    print('X_train shape is', X_train.shape)\n",
    "    print('y_train shape is', ylog_train.shape)\n",
    "    print('X_val shape is', X_val.shape)\n",
    "    print('y_val shape is', ylog_val.shape)\n",
    "    print('X_test shape is', X_test.shape)\n",
    "    \n",
    "    dtrain_all = xgb.DMatrix(X_train_all, ylog_train_all, feature_names=df_columns)\n",
    "    dtrain = xgb.DMatrix(X_train, ylog_train, feature_names=df_columns)\n",
    "    dval = xgb.DMatrix(X_val, ylog_val, feature_names=df_columns)\n",
    "    dtest = xgb.DMatrix(X_test, feature_names=df_columns)\n",
    "\n",
    "    xgb_params = {\n",
    "        'eta': 0.05,\n",
    "        'max_depth': 5,\n",
    "        'subsample': 0.7,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'objective': 'reg:linear',\n",
    "        'eval_metric': 'rmse',\n",
    "        'silent': 1,\n",
    "        'seed':5\n",
    "    }\n",
    "\n",
    "    num_round = 1000\n",
    "    xgb_params['nthread'] = 24\n",
    "    evallist = [(dval, 'eval')]\n",
    "\n",
    "    bst = xgb.train(xgb_params, dtrain, num_round, evallist, early_stopping_rounds=40, verbose_eval=10)\n",
    "    \n",
    "    if (isLog1p):\n",
    "        train_rmse = mean_squared_error(ylog_train, bst.predict(dtrain))\n",
    "        val_rmse = mean_squared_error(ylog_val, bst.predict(dval))\n",
    "    else:\n",
    "        train_rmse = mean_squared_error(np.log1p(ylog_train), np.log1p(bst.predict(dtrain)))\n",
    "        val_rmse = mean_squared_error(np.log1p(ylog_val), np.log1p(bst.predict(dval)))\n",
    "    print 'train_rmse =', np.sqrt(train_rmse), ', val_rmse =', np.sqrt(val_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
