---
title: "Cleaning the data using Lat and Longitude"
author: "Nigel Carpenter"
date: "May, 2017"
output: 
  html_document: 
    code_folding: "show"
    highlight: haddock
    theme: readable
    fig_width: 9
    fig_height: 6
---
# Introduction

In my earlier kernels I illustrated an approach to identify a property's location and code to extract latitude and longitude for all locations
(https://www.kaggle.com/nigelcarpenter/lat-and-longitude-for-all-locations).

I now take that further and provide code that shows how the lat lon location data can be used to apply a degree of automated and intelligent cleansing of this dataset.

Once again I ask that if you find this useful that you please give it an upvote. 

# Libraries and Data

## Libraries
```{r package loading, message=FALSE, warning=FALSE}

# == General == #
library(data.table) # read data with fread()
library(DT)         # display interactive tables

# == Spatial == #
#library(rgdal)
#library(rgeos)
#library(sp)
#library(raster)

```
## Data

The focus of this script will be on cleansing the property details data. There are a lot of NA's and spurious 0, 1, 99 values that common sense says cannot be correct. My hope is that by removing and correcting this the accuracy of predictions will be improved.

So let's start by reading in the property related factors, the latitudes and longitudes and creating a summary feature counting the number of properties at the location.

```{r, message=FALSE, warning=FALSE}


lst_property_features <- c("id", "timestamp", "full_sq", "life_sq", "floor",                                
                           "max_floor", "material", "build_year", "num_room", "kitch_sq",                             
                           "state", "product_type", "sub_area")

# Load CSV files
data.train <- fread("../input/sberbank-russian-housing-market/train.csv", sep=",", na.strings = "NA", select = c(lst_property_features, "price_doc"))
data.test <- fread("../input/sberbank-russian-housing-market/test.csv", sep=",", na.strings = "NA", select = lst_property_features)

data.train[, train := 1]
data.test[, train := 0]
data.test[, price_doc := 0]


# Load location lat lon files
data.train.latlon <- fread("../input/sberbankmoscowroads/train_lat_lon.csv", sep=",", na.strings = "NA")
data.test.latlon <- fread("../input/sberbankmoscowroads/test_lat_lon.csv", sep=",", na.strings = "NA")

# Combine data.tables
data.all <- rbind(data.train, data.test)
data.latlon <- rbind(data.train.latlon, data.test.latlon)

setkey(data.all, id)
setkey(data.latlon, id)

data.all <- data.all[data.latlon]

# tidy up
rm(data.train, data.test, data.train.latlon, data.test.latlon)

# features

# 1 how many poperties at this location
data.latlon.sum <- data.latlon[,.(count = .N) , by=key]
setkey(data.latlon.sum, key)
setkey(data.latlon, key)
setkey(data.all, key)

data.latlon <- data.latlon[data.latlon.sum]
data.all <- data.all[data.latlon.sum]

```

# Cleaning Max_Floor

The data has a large number of properties with max_floor that would be considered extreme. 

```{r, message=FALSE, warning=FALSE}
data.all[, max_floor_orig := max_floor]
summary(data.all$max_floor_orig)

hist(data.all$max_floor_orig, breaks =117)


```
## Simple anomalies

The tallest apartment block in Moscow is Triumph-Palace with 57 floors https://en.wikipedia.org/wiki/Triumph_Palace.
It is also very unlikely to have properties with 1 or fewer floors in highrise Moscow.

Further, properties where the actual floor is greater than the max_floor must be an error. It's not clear if the error is in the max_floor or floor field so we will mark these rows with an error flag.

```{r, message=FALSE, warning=FALSE}

# set any rows > 57 to NA
data.all[max_floor_orig > 57, max_floor := NA]

# max_floor = 0 has to mean not known so set to NA
data.all[max_floor_orig ==0, max_floor := NA]

# max_floor = 1 is unlikely in high rise Moscow
data.all[floor>1 & max_floor_orig ==1, max_floor := NA]

hist(data.all$max_floor, breaks =117)

```

Further, properties where the actual floor is greater than the max_floor must be an error.
It's not clear if the error is in the max_floor or floor field so we will mark these rows with an error flag.

The summary and histogram after this simple cleansing already look more sensible.

```{r, message=FALSE, warning=FALSE}

# now what about properties where floor > max_floor
datatable(data.all[floor> max_floor, -"key", with = FALSE], style="bootstrap", class="table-condensed", options = list(dom = 'tp',scrollX = TRUE))

# interesting lots of impossibles and 1 seems to be a not-known flag

# lets create an error flag so we can remove them for now
data.all[, err_flag := 0]
data.all[floor> max_floor | is.na(max_floor) , err_flag := 1]

data.all[floor==1 &  max_floor ==1 & build_year==1 & full_sq==1, err_flag := 1]

summary(data.all[err_flag == 0, max_floor])
hist(data.all[err_flag == 0, max_floor], breaks =117)


```


## Location based anomalies

Now I move onto to more sophisticated fixes. I will now look at properties by locaiton and identify locations with large discrepancies in other features.


```{r, message=FALSE, warning=FALSE}

# create a R function for Mode
Mode = function(x){ 
  ta = table(x)
  tam = max(ta)
  if (all(ta == tam))
    mod = NA
  else
    if(is.numeric(x))
      mod = as.numeric(names(ta)[ta == tam])
  else
    mod = names(ta)[ta == tam]
  return(mod)
}


# some complications with data.table require as.double() wrapping
tmp <- data.all[err_flag == 0, .(max_floor_avg = mean(max_floor, na.rm = TRUE),
                                 max_floor_max = max(max_floor, na.rm = TRUE),
                                 max_floor_min = min(max_floor, na.rm = TRUE),
                                 max_floor_med = as.double(median(max_floor, na.rm = TRUE)),
                                 max_floor_mod = as.double(Mode(max_floor)),
                                 max_floor_sd = sd(max_floor, na.rm = TRUE),
                                 count = .N,
                                 count.all = mean(count)), by= key]



datatable(tmp[,-"key", with = FALSE], style="bootstrap", class="table-condensed", options = list(dom = 'tp',scrollX = TRUE))

```

# Data anomaly: red circle in the Red Square!

Now I want to illustrate some concerns about the quality of location data. I'll extract all the locations for the sub_area of Tverskoe and plot them on a map. The colou of the circle will reflect the number of properties sold at that location. 

We see a red circle in the Red Square! Supposedly over 700 properties have been sold in this exact location.

```{r, echo = FALSE, message= FALSE, warning=FALSE}


```
